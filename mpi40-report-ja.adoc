= MPI: A Message-Passing Interface Standard Version 4.0

:doctype: book
:sectnums:
:sectnumlevels: 3
:toc: left
:toclevels: 4

== List of Figures

== List of Tables

== Acknowledgements

== Chapter 1 Introduction to MPI

=== 1.1 Overview and Goals

=== 1.2 Background of MPI-1.0

=== 1.3 Background of MPI-1.1, MPI-1.2, and MPI-2.0

=== 1.4 Background of MPI-1.3 and MPI-2.1

=== 1.5 Background of MPI-2.2

=== 1.6 Background of MPI-3.0

=== 1.7 Background of MPI-3.1

=== 1.8 Background of MPI-4.0

=== 1.9 Who Should Use This Standard?

=== 1.10 What Platforms Are Targets for Implementation?

=== 1.11 What Is Included in the Standard?

=== 1.12 What Is Not Included in the Standard?

=== 1.13 Organization of This Document

== Chapter 2 MPI Terms and Conventions

=== 2.1 Document Notation

=== 2.2 Naming Conventions

=== 2.3 Procedure Specification

=== 2.4 Semantic Terms

==== 2.4.1 MPI Operations

==== 2.4.2 MPI Procedures

==== 2.4.3 MPI Datatypes

=== 2.5 Datatypes

==== 2.5.1 Opaque Objects

MPI manages system memory that is used for buffering messages and for storing internal representations of various MPI objects such as groups, communicators, datatypes, etc.
This memory is not directly accessible to the user, and objects stored there are opaque: their size and shape is not visible to the user.
Opaque objects are accessed via handles, which exist in user space.
MPI procedures that operate on opaque objects are passed handle arguments to access these objects.
In addition to their use by MPI calls for object access, handles can participate in assignments and comparisons.

MPIは、メッセージのバッファリングや、グループ、コミュニケータ、データ型などの様々なMPIオブジェクトの内部表現を格納するために使用されるシステムメモリを管理します。 
このメモリはユーザが直接アクセスできるものではなく、そこに格納されたオブジェクトは不透明です。
不透明なオブジェクトは、ユーザ空間に存在するハンドルを介してアクセスされます。
不透明オブジェクトを操作するMPI手続きは、これらのオブジェクトにアクセスするためにハンドル引数を渡されます。
MPI呼び出しによるオブジェクトへのアクセスに加えて、ハンドルは代入や比較に参加することができます。

In Fortran with USE mpi or INCLUDE 'mpif.h', all handles have type INTEGER.
In Fortran with USE mpi_f08, and in C, a different handle type is defined for each category of objects.
With Fortran USE mpi_f08, the handles are defined as Fortran BIND(C) derived types that consist of only one element INTEGER :: MPI_VAL.
The internal handle value is identical to the Fortran INTEGER value used in the mpi module and mpif.h.
The operators ".EQ.", ".NE.", "==" and "/=" are overloaded to allow the comparison of these handles.
The type names are identical to the names in C, except that they are not case sensitive.

USE mpi または INCLUDE 'mpif.h' を使用する Fortran では、すべてのハンドルの型は INTEGER です。
USE mpi_f08 を使用する Fortran および C では、オブジェクトのカテゴリごとに異なるハンドル型が定義されます。
USE mpi_f08 を使用する Fortran では、ハンドルは1つの要素 INTEGER :: MPI_VAL です。
内部ハンドルの値は mpi モジュールと mpif.h で使用される Fortran INTEGER と同じです。
演算子 ".EQ.", ".NE.", "==", "/=" はこれらのハンドルの比較を可能にするためにオーバーロードされます。
型名は、大文字と小文字を区別しないことを除いて、C言語の型名と同じです。

[source,fortran]
----
TYPE, BIND(C) :: MPI_Comm
  INTEGER :: MPI_VAL
END TYPE MPI_Comm
----

The C types must support the use of the assignment and equality operators.

Cの型は、代入演算子と等号演算子の使用をサポートしなければなりません。

NOTE: *Advice to implementors.*
In Fortran, the handle can be an index into a table of opaque objects in a system table; in C it can be such an index or a pointer to the object.
(End of advice to implementors.)

NOTE: *実装者へのアドバイス*
Fortranでは、ハンドルはシステム・テーブル内の不透明オブジェクトのテーブルへのインデックスです。
(実装者へのアドバイスの終わり)

NOTE: *Rationale.*
Since the Fortran integer values are equivalent, applications can easily convert MPI handles between all three supported Fortran methods.
For example, an integer communicator handle COMM can be converted directly into an exactly equivalent mpi_f08 communicator handle named comm_f08 by comm_f08%MPI_VAL=COMM, and vice versa.
The use of the INTEGER defined handles and the BIND(C) derived type handles is different: Fortran 2003 (and later) define that BIND(C) derived types can be used within user defined common blocks, but it is up to the rules of the companion C compiler how many numerical storage units are used for these BIND(C) derived type handles.
Most compilers use one unit for both, the INTEGER handles and the handles defined as BIND(C) derived types.
(End of rationale.)

NOTE: *根拠*
Fortranの整数値は等価であるため、アプリケーションはサポートされている3つのFortranメソッド間でMPIハンドルを簡単に変換することができます。
例えば、整数値のコミュニケータハンドルCOMMはcomm_f08%MPI_VAL=COMMによってcomm_f08 という名前の mpi_f08 コミュニケータハンドルに直接変換することができます。
INTEGER定義ハンドルとBIND(C)派生型ハンドルの使用方法は異なります: Fortran 2003(およびそれ以降)では、BIND(C)派生型はユーザ定義の共通ブロック内で使用できると定義されていますが、これらのBIND(C)派生型ハンドルに何個の数値記憶ユニットを使用するかはコンパイラの規則次第です。
ほとんどのコンパイラは、INTEGERハンドルとBIND©派生型として定義されたハンドルの両方に1単位を使用します。
(根拠終わり)

NOTE: *Advice to users.*
If a user wants to substitute mpif.h or the mpi module by the mpi_f08 module and the application program stores a handle in a Fortran common block then it is necessary to change the Fortran support method in all application routines that use this common block, because the number of numerical storage units of such a handle can be different in the two modules.
(End of advice to users.)

NOTE: *ユーザへのアドバイス*
もし、ユーザが mpif.h または mpi モジュールを mpi_f08 モジュールで置き換えたい場合で、アプリケーションプログラムが Fortran 共通ブロックにハンドルを格納する場合、この共通ブロックを使用するすべてのアプリケーションルーチンで Fortran サポートメソッドを変更する必要があります。
(ユーザーへのアドバイスの終わり)

Opaque objects are allocated and deallocated by calls that are specific to each object type.
These are listed in the sections where the objects are described.
The calls accept a handle argument of matching type.
In an allocate call this is an OUT argument that returns a valid reference to the object.
In a call to deallocate this is an INOUT argument which returns with an "invalid handle" value.
MPI provides an "invalid handle" constant for each object type.
Comparisons to this constant are used to test for validity of the handle.

不透明オブジェクトは、各オブジェクトタイプに固有の呼び出しによって割り当てと割り当て解除が行われます。
これらの呼び出しは、オブジェクトが説明されているセクションにリストされています。
呼び出しは、型が一致する handle 引数を受け取ります。
allocate呼び出しでは、これはオブジェクトへの有効な参照を返すOUT引数です。
deallocate呼び出しでは、これは "invalid handle"値で返すINOUT引数です。
MPIは各オブジェクト型に対して "無効なハンドル"定数を提供します。
この定数との比較がハンドルの有効性をテストするために使用されます。

A call to a deallocate routine invalidates the handle and marks the object for deallocation.
The object is not accessible to the user after the call. However, MPI need not deallocate the object immediately.
Any operation pending (at the time of the deallocate) that involves this object will complete normally; the object will be deallocated afterwards.

deallocateルーチンを呼び出すと、ハンドルは無効になり、そのオブジェクトは割り当てが解除されます。
この呼び出しの後、ユーザはオブジェクトにアクセスできなくなります。しかし、MPIは直ちにオブジェクトを解放する必要はありません。
deallocateされた時点で保留されている、このオブジェクトに関係する操作はすべて正常に完了し、オブジェクトはその後にdeallocateされます。

An opaque object and its handle are significant only at the process where the object was created and cannot be transferred to another process.
MPI provides certain predefined opaque objects and predefined, static handles to these objects.
The user must not free such objects.

不透明オブジェクトとそのハンドルは、そのオブジェクトが作成されたプロセスでのみ重要であり、他のプロセスに転送することはできません。
MPIは、特定の定義済み不透明オブジェクトと、これらのオブジェクトへの定義済み静的ハンドルを提供します。
ユーザはそのようなオブジェクトを解放してはいけません。

NOTE: *Rationale.*
This design hides the internal representation used for MPI data structures, thus allowing similar calls in C and Fortran.
It also avoids conflicts with the typing rules in these languages, and easily allows future extensions of functionality.
The mechanism for opaque objects used here loosely follows the POSIX Fortran binding standard. +
The explicit separation of handles in user space and objects in system space allows space-reclaiming and deallocation calls to be made at appropriate points in the user program.
If the opaque objects were in user space, one would have to be very careful not to go out of scope before any pending operation requiring that object completed.
The specified design allows an object to be marked for deallocation, the user program can then go out of scope, and the object itself still persists until any pending operations are complete. +
The requirement that handles support assignment/comparison is made since such operations are common.
This restricts the domain of possible implementations.
The alternative in C would have been to allow handles to have been an arbitrary, opaque type.
This would force the introduction of routines to do assignment and comparison, adding complexity, and was therefore ruled out.
In Fortran, the handles are defined such that assignment and comparison are available through the operators of the language or overloaded versions of these operators. (End of rationale.)

NOTE: *根拠*
この設計は、MPIデータ構造に使用される内部表現を隠蔽するため、CやFortranでも同様の呼び出しが可能です。
また、これらの言語の型付け規則との衝突を回避し、将来的な機能拡張を容易にします。
ここで使用されている不透明オブジェクトのメカニズムは、POSIX Fortranバインディング標準に緩く従っています。 +
ユーザー空間のハンドルとシステム空間のオブジェクトを明示的に分離することで、ユーザープログラムの適切な箇所で空間奪還と解放の呼び出しを行うことができます。
不透明なオブジェクトがユーザー空間にあった場合、そのオブジェクトを必要とする保留中の操作が完了する前にスコープ外に出ないように、細心の注意を払わなければなりません。
指定された設計では、オブジェクトに割り当て解除のマークを付けることができ、ユーザー・プログラムはスコープ外に出ることができます。 +
ハンドルの割り当て/比較をサポートするという要件は、そのような操作が一般的であるためです。
これにより、実装可能な領域が制限されます。
C言語の代替案としては、ハンドルを任意の不透明な型にすることも可能だったと思います。
この場合、代入と比較を行うルーチンを導入しなければならなくなり、複雑さが増すため、除外されました。
Fortranでは、ハンドルの代入と比較は、その言語の演算子か、これらの演算子のオーバーロード版で利用できるように定義されています。(根拠終わり)

NOTE: *Advice to users.*
A user may accidentally create a dangling reference by assigning to a handle the value of another handle, and then deallocating the object associated with these handles.
Conversely, if a handle variable is deallocated before the associated object is freed, then the object becomes inaccessible (this may occur, for example, if the handle is a local variable within a subroutine, and the subroutine is exited before the associated object is deallocated).
It is the user’s responsibility to avoid adding or deleting references to opaque objects, except as a result of MPI calls that allocate or deallocate such objects. (End of advice to users.)

NOTE: *ユーザへのアドバイス*
ユーザは、ハンドルに別のハンドルの値を代入し、その後これらのハンドルに関連付けられたオブジェクトを解放することで、誤ってぶら下がり参照を作成する可能性があります。
逆に、関連するオブジェクトが解放される前にハンドル変数が解放されると、そのオブジェクトはアクセスできなくなります（例えば、ハンドルがサブルーチン内のローカル変数であり、関連するオブジェクトが解放される前にサブルーチンが終了した場合などに、このような現象が発生する可能性があります）。
不透明なオブジェクトへの参照を追加したり削除したりしないようにするのは、そのようなオブジェクトを割り当てたり解放したりするMPI呼び出しの結果以外では、ユーザの責任です。(ユーザへの忠告を終わります)。

NOTE: *Advice to implementors.*
The intended semantics of opaque objects is that opaque objects are separate from one another; each call to allocate such an object copies all the information required for the object.
Implementations may avoid excessive copying by substituting referencing for copying.
For example, a derived datatype may contain references to its components, rather than copies of its components; a call to MPI_COMM_GROUP may return a reference to the group associated with the communicator, rather than a copy of this group.
In such cases, the implementation must maintain reference counts, and allocate and deallocate objects in such a way that the visible effect is as if the objects were copied. (End of advice to implementors.)

NOTE: *実装者へのアドバイス*
不透明オブジェクトの意図されたセマンティクスは、不透明オブジェクトは互いに分離しているということです。そのようなオブジェクトを割り当てるための各呼び出しは、そのオブジェクトに必要なすべての情報をコピーします。
実装では、コピーの代わりに参照を使用することで、過剰なコピーを避けることができます。
MPI_COMM_GROUP を呼び出すと、そのグループのコピーではなく、コミュニケータに関連付けられたグループへの参照が返されます。
このような場合、実装は参照カウントを維持し、オブジェクトがコピーされたかのように見えるようにオブジェクトを割り当てたり、割り当て解除したりしなければなりません。(実装者へのアドバイスはここまで）。


==== 2.5.2 Array Arguments

==== 2.5.3 State

==== 2.5.4 Named Constants

MPI procedures sometimes assign a special meaning to a special value of a basic type argument; e.g., tag is an integer-valued argument of point-to-point communication operations, with a special wild-card value, MPI_ANY_TAG.
Such arguments will have a range of regular values, which is a proper subrange of the range of values of the corresponding basic type; special values (such as MPI_ANY_TAG) will be outside the regular range.
The range of regular values, such as tag, can be queried using environmental inquiry functions, see Chapter 9.
The range of other values, such as source, depends on values given by other MPI routines (in the case of source it is the communicator size).

MPI手続きは、基本型の引数の特別な値に特別な意味を割り当てることがあります。例えば、tagはポイントツーポイント通信操作の整数値の引数で、MPI_ANY_TAGという特別なワイルドカード値を持ちます。
このような引数には、対応する基本型の値の範囲の適切な部分範囲である正規値の範囲があります。特殊な値(MPI_ANY_TAGなど)は正規の範囲外となります。
tagのような正規値の範囲は、環境問い合わせ関数を使用して問い合わせることができます。
source のような他の値の範囲は、他の MPI ルーチンで与えられた値に依存します (source の場合はコミュニケータサイズです)。

MPI also provides predefined named constant handles, such as MPI_COMM_WORLD.

MPI は MPI_COMM_WORLD のような定義済みの名前付き定数ハンドルも提供します。

All named constants, with the exceptions noted below for Fortran, can be used in initialization expressions or assignments, but not necessarily in array declarations or as labels in C switch or Fortran select/case statements.
This implies named constants to be link-time but not necessarily compile-time constants.
The named constants listed below are required to be compile-time constants in both C and Fortran.
These constants do not change values during execution.
Opaque objects accessed by constant handles are defined and do not change value between MPI initialization (MPI_INIT) and MPI completion (MPI_FINALIZE).
The handles themselves are constants and can be also used in initialization expressions or assignments.

すべての名前付き定数は、Fortranの例外を除いて、初期化式や代入で使用することができますが、配列宣言やCのswitch文やFortranのselect/case文のラベルとして使用することはできません。
これは、名前付き定数がリンク時定数であることを意味しますが、コンパイル時定数であるとは限りません。
以下に挙げる名前付き定数は、CでもFortranでもコンパイル時定数であることが要求されます。
これらの定数は実行中に値が変わることはありません。
定数ハンドルによってアクセスされる不透明オブジェクトは、MPI の初期化 (MPI_INIT) から MPI の完了 (MPI_FINALIZE) までの間、値が変化しないように定義されています。
ハンドル自体は定数であり、初期化式や代入で使用することもできます。

The constants that are required to be compile-time constants (and can thus be used for array length declarations and labels in C switch and Fortran case/select statements) are:

コンパイル時定数として要求される定数(配列の長さの宣言やCのswitchやFortranのcase/select文のラベルに使用できる)は以下の通りです:

[source]
----
MPI_MAX_PROCESSOR_NAME
MPI_MAX_LIBRARY_VERSION_STRING
MPI_MAX_ERROR_STRING
MPI_MAX_DATAREP_STRING
MPI_MAX_INFO_KEY
MPI_MAX_INFO_VAL
MPI_MAX_OBJECT_NAME
MPI_MAX_PORT_NAME
MPI_VERSION
MPI_SUBVERSION
MPI_F_STATUS_SIZE (C only)
MPI_STATUS_SIZE (Fortran only)
MPI_ADDRESS_KIND (Fortran only)
MPI_COUNT_KIND (Fortran only)
MPI_INTEGER_KIND (Fortran only)
MPI_OFFSET_KIND (Fortran only)
MPI_SUBARRAYS_SUPPORTED (Fortran only)
MPI_ASYNC_PROTECTS_NONBLOCKING (Fortran only)
----

The constants that cannot be used in initialization expressions or assignments in Fortran are as follows:

Fortranの初期化式や代入で使用できない定数は以下の通りです:

[source]
----
MPI_BOTTOM
MPI_STATUS_IGNORE
MPI_STATUSES_IGNORE
MPI_ERRCODES_IGNORE
MPI_IN_PLACE
MPI_ARGV_NULL
MPI_ARGVS_NULL
MPI_UNWEIGHTED
MPI_WEIGHTS_EMPTY
----

NOTE: *Advice to implementors.*
In Fortran the implementation of these special constants may require the use of language constructs that are outside the Fortran standard.
Using special values for the constants (e.g., by defining them through PARAMETER statements) is not possible because an implementation cannot distinguish these values from valid data.
Typically, these constants are implemented as predefined static variables (e.g., a variable in an MPI-declared COMMON block), relying on the fact that the target compiler passes data by address. 
Inside the subroutine, this address can be extracted by some mechanism outside the Fortran standard (e.g., by Fortran extensions or by implementing the function in C).
(End of advice to implementors.)

NOTE: *実装者へのアドバイス*
Fortranでは、これらの特殊な定数の実装は、Fortran標準外の言語構造を使用する必要があるかもしれません。
実装がこれらの値を有効なデータと区別することができないため、定数に特別な値を使用する（例えば、PARAMETER文で定義する）ことはできません。
通常、これらの定数は、ターゲットコンパイラがアドレスによってデータを渡すという事実に依存して、定義済みの静的変数（例えば、MPI宣言されたCOMMONブロック内の変数）として実装されます。
サブルーチン内部では、このアドレスはFortran標準外の何らかのメカニズム（例えば、Fortranの拡張やCでの関数の実装）によって抽出することができます。
(実装者へのアドバイスの終わり)


==== 2.5.5 Choice

==== 2.5.6 Absolute Addresses and Relative Address Displacements

==== 2.5.7 File Offsets

==== 2.5.8 Counts

=== 2.6 Language Binding

==== 2.6.1 Deprecated and Removed Interfaces

==== 2.6.2 Fortran Binding Issues

==== 2.6.3 C Binding Issues

==== 2.6.4 Functions and Macros

=== 2.7 Processes

=== 2.8 Error Handling

MPI provides the user with reliable message transmission. A message sent is always received correctly, and the user does not need to check for transmission errors, time-outs, or other error conditions.
In other words, MPI does not provide mechanisms for dealing with transmission failures in the communication system.
If the MPI implementation is built on an unreliable underlying mechanism, then it is the job of the implementor of the MPI subsystem to insulate the user from this unreliability, and to reflect only unrecoverable transmission failures.
Whenever possible, such failures will be reflected as errors in the relevant communication call.

MPIは信頼性の高いメッセージ伝送をユーザーに提供します。
送信されたメッセージは常に正しく受信され、ユーザは送信エラーやタイムアウトなどのエラー状態をチェックする必要がありません。
言い換えれば、MPIは通信システムにおける伝送障害に対処する機構を提供しません。
もしMPIの実装が信頼性の低い機構の上に構築されているのであれば、MPIサブシステムの実装者は、この信頼性の低さからユーザを隔離し、回復不可能な伝送障害だけを反映させるのが仕事です。
可能な限り、そのような失敗は関連する通信呼び出しのエラーとして反映されます。

Similarly, MPI itself provides no mechanisms for handling MPI process failures, that is, when an MPI process unexpectedly and permanently stops communicating (e.g., a software or hardware crash results in an MPI process terminating unexpectedly).

同様に、MPI自身はMPIプロセスの障害、つまりMPIプロセスが予期せず永続的に通信を停止した場合（例えば、ソフトウェアやハードウェアのクラッシュによりMPIプロセスが予期せず終了した場合）を処理するメカニズムを提供していません。

Of course, MPI programs may still be erroneous.
A program error can occur when an MPI call is made with an incorrect argument (non-existing destination in a send operation, buffer too small in a receive operation, etc.).
This type of error would occur in any implementation.
In addition, a resource error may occur when a program exceeds the amount of available system resources (number of pending messages, system buffers, etc.).

もちろん、MPIプログラムにもエラーはあります。
プログラムのエラーは、MPIコールに不正な引数（送信操作で宛先が存在しない、受信操作でバッファが小さすぎる、など）が指定された場合に発生します。
この種のエラーはどのような実装でも発生します。
さらに、リソースエラーは、プログラムが利用可能なシステムリソースの量（保留中のメッセージの数、システムバッファなど）を超えた場合に発生する可能性があります。

The occurrence of this type of error depends on the amount of available resources in the system and the resource allocation mechanism used; this may differ from system to system.
A high-quality implementation will provide generous limits on the important resources so as to alleviate the portability problem this represents.

この種のエラーの発生は、システムで利用可能なリソースの量と、使用されるリソース割り当てメカニズムに依存します。
高品質な実装では、重要なリソースに寛大な制限を設け、これが示す移植性の問題を緩和します。

In C and Fortran, almost all MPI calls return a code that indicates successful completion of the operation.
Whenever possible, MPI calls return an error code if an error occurred during the call.
By default, an error detected during the execution of the MPI library causes the parallel computation to abort, except for file operations.
However, MPI provides mechanisms for users to change this default and to handle recoverable errors. 
The user may specify that no error is fatal, and handle error codes returned by MPI calls by themselves.
Also, the user may provide user-defined error-handling routines, which will be invoked whenever an MPI call returns abnormally.
The MPI error handling facilities are described in Section 9.3.

CおよびFortranでは、ほとんどすべてのMPIコールは操作の正常終了を示すコードを返します。
MPIコールは可能な限り、コール中にエラーが発生した場合にエラーコードを返します。
デフォルトでは、MPIライブラリの実行中に検出されたエラーは、ファイル操作を除いて並列計算を中断させます。
しかし、MPIはユーザがこのデフォルトを変更し、回復可能なエラーを処理するための機構を提供します。
ユーザは、致命的なエラーでないことを指定し、MPIコールから返されるエラーコードを自分で処理することができます。
また、ユーザ定義エラー処理ルーチンを用意し、MPIコールが異常終了したときに呼び出すこともできます。
MPIエラー処理機能については9.3節で説明します。

Several factors limit the ability of MPI calls to return with meaningful error codes when an error occurs.
MPI may not be able to detect some errors; other errors may be too expensive to detect in normal execution mode; some faults (e.g., memory faults) may corrupt the state of the MPI library and its outputs; finally some errors may be "catastrophic" and may prevent MPI from returning control to the caller.
On the other hand, some errors may be detected after the associated operation has completed; some errors may not have a communicator, window, or file on which an error may be raised.
In such cases, these errors will be raised on the communicator MPI_COMM_SELF when using the World Model (see Section 11.2).
When MPI_COMM_SELF is not initialized (i.e., before MPI_INIT / MPI_INIT_THREAD, after MPI_FINALIZE, or when using the Sessions Model exclusively) the error raises the initial error handler (set during the launch operation, see 11.8.4).
The Sessions Model is described in Section 11.3.

MPIコールがエラー発生時に意味のあるエラーコードを返すことを制限するいくつかの要因があります。
あるエラー(例えば、メモリエラー)はMPIライブラリとその出力の状態を壊してしまう可能性があります。
一方、エラーの中には、関連する操作が完了した後に検出されるものもあります。
また、エラーが発生するようなコミュニケータ、ウィンドウ、ファイルが存在しないものもあります。
そのような場合、ワールドモデル(セクション11.2を参照)を使用する場合、これらのエラーはコミュニケータMPI_COMM_SELF上で発生します。
MPI_COMM_SELF が初期化されていない場合 (MPI_INIT / MPI_INIT_THREAD の前、MPI_FINALIZE の後、またはセッションズモデルのみを使用している場合)、エラーは初期エラーハンドラ (起動操作中に設定されます。11.8.4 参照) を発生させます。
セッションズ・モデルについてはセクション11.3で説明します。

An example of such a case arises because of the nature of asynchronous communications: MPI calls may initiate operations that continue asynchronously after the call returned.
Thus, the operation may return with a code indicating successful completion, yet later cause an error to be raised.
If there is a subsequent call that relates to the same operation (e.g., a call that verifies that an asynchronous operation has completed) then the error argument associated with this call will be used to indicate the nature of the error.
In a few cases, the error may occur after all calls that relate to the operation have completed, so that no error value can be used to indicate the nature of the error (e.g., an error on the receiver in a send with the ready mode).

非同期通信の性質上、このようなケースが発生する: MPI呼び出しは、呼び出しが返った後も非同期で継続する操作を開始することがあります。
MPIコールは、コールが返った後も非同期に継続するオペレーションを開始することがあります。
したがって、オペレーションが正常に完了したことを示すコードで返ったにもかかわらず、後でエラーが発生することがあります。
同じ操作に関連する後続の呼び出し(例えば、非同期操作が完了したことを確認する呼び出し)がある場合、この呼び出しに関連するエラー引数は、エラーの性質を示すために使用されます。
場合によっては、操作に関連するすべての呼が完了した後にエラーが発生し、 エラー値を使用してエラーの性質を示すことができないことがある(たとえば、 レディモードでの送信における受信側のエラー)。

This document does not specify the state of a computation after an erroneous MPI call has occurred.
The desired behavior is that a relevant error code be returned, and the effect of the error be localized to the greatest possible extent.
E.g., it is highly desirable that an erroneous receive call will not cause any part of the receiver's memory to be overwritten, beyond the area specified for receiving the message.

この文書では、誤ったMPIコールが発生した後の計算の状態については規定しません。
望ましい動作は、関連するエラーコードが返され、エラーの影響が可能な限り局所化されることです。
例えば、誤った受信呼び出しが発生しても、メッセージを受信するために指定された領域を超えて、受信側のメモリの一部が上書きされないことが非常に望ましいです。

Implementations may go beyond this document in supporting in a meaningful manner MPI calls that are defined here to be erroneous.
For example, MPI specifies strict type matching rules between matching send and receive operations: it is erroneous to send a floating point variable and receive an integer.
Implementations may go beyond these type matching rules, and provide automatic type conversion in such situations.
It will be helpful to generate warnings for such nonconforming behavior.

実装は、ここで誤りと定義されているMPIコールを意味のある形でサポートするために、このドキュメントを越えてもよい。
例えば、MPIは送信操作と受信操作のマッチングに厳格な型マッチングルールを規定しています: 浮動小数点変数を送信して整数を受信することは誤りです。
実装は、これらの型照合ルールを超えて、そのような状況で自動的な型変換を提供するかもしれません。
そのような不適合な動作に対する警告を生成することは有益だと思います。

MPI defines a way for users to create new error codes as defined in Section 9.5.

MPIは、セクション9.5で定義されているように、ユーザが新しいエラーコードを作成する方法を定義しています。


=== 2.9 Implementation Issues

==== 2.9.1 Independence of Basic Runtime Routines

==== 2.9.2 Interaction with Signals

=== 2.10 Examples

== Chapter 3 Point-to-Point Communication

=== 3.1 Introduction

=== 3.2 Blocking Send and Receive Operations

==== 3.2.1 Blocking Send

==== 3.2.2 Message Data

==== 3.2.3 Message Envelope

==== 3.2.4 Blocking Receive

==== 3.2.5 Return Status

==== 3.2.6 Passing MPI_STATUS_IGNORE for Status

==== 3.2.7 Blocking Send-Receive

=== 3.3 Datatype Matching and Data Conversion

==== 3.3.1 Type Matching Rules

===== 3.3.1.1 Type MPI_CHARACTER

==== 3.3.2 Data Conversion

=== 3.4 Communication Modes

=== 3.5 Semantics of Point-to-Point Communication

=== 3.6 Buffer Allocation and Usage

==== 3.6.1 Model Implementation of Buffered Mode

=== 3.7 Nonblocking Communication

==== 3.7.1 Communication Request Objects

==== 3.7.2 Communication Initiation

==== 3.7.3 Communication Completion

==== 3.7.4 Semantics of Nonblocking Communications

==== 3.7.5 Multiple Completions

==== 3.7.6 Non-Destructive Test of status

==== 3.8 Probe and Cancel

==== 3.8.1 Probe

==== 3.8.2 Matching Probe

==== 3.8.3 Matched Receives

==== 3.8.4 Cancel

=== 3.9 Persistent Communication Requests

=== 3.10 Null Processes

== Chapter 4 Partitioned Point-to-Point Communication

=== 4.1 Introduction

=== 4.2 Semantics of Partitioned Point-to-Point Communication

==== 4.2.1 Communication Initialization and Starting with Partitioning

==== 4.2.2 Communication Completion under Partitioning

==== 4.2.3 Semantics of Communications in Partitioned Mode

=== 4.3 Partitioned Communication Examples

==== 4.3.1 Partition Communication with Threads/Tasks Using OpenMP 4.0 or later

==== 4.3.2 Send-only Partitioning Example with Tasks and OpenMP version 4.0 or later

==== 4.3.3 Send and Receive Partitioning Example with OpenMP version 4.0 or later

== Chapter 5 Datatypes

=== 5.1 Derived Datatypes

==== 5.1.1 Type Constructors with Explicit Addresses

==== 5.1.2 Datatype Constructors

==== 5.1.3 Subarray Datatype Constructor

==== 5.1.4 Distributed Array Datatype Constructor

==== 5.1.5 Address and Size Functions

==== 5.1.6 Lower-Bound and Upper-Bound Markers

==== 5.1.7 Extent and Bounds of Datatypes

==== 5.1.8 True Extent of Datatypes

==== 5.1.9 Commit and Free

==== 5.1.10 Duplicating a Datatype

==== 5.1.11 Use of General Datatypes in Communication

==== 5.1.12 Correct Use of Addresses

==== 5.1.13 Decoding a Datatype

==== 5.1.14 Examples

=== 5.2 Pack and Unpack

=== 5.3 Canonical MPI_PACK and MPI_UNPACK

== Chapter 6 Collective Communication

=== 6.1 Introduction and Overview

=== 6.2 Communicator Argument

==== 6.2.1 Specifics for Intra-Communicator Collective Operations

==== 6.2.2 Applying Collective Operations to Inter-Communicators

==== 6.2.3 Specifics for Inter-Communicator Collective Operations

=== 6.3 Barrier Synchronization

=== 6.4 Broadcast

==== 6.4.1 Example using MPI_BCAST

=== 6.5 Gather

==== 6.5.1 Examples using MPI_GATHER, MPI_GATHERV

=== 6.6 Scatter

==== 6.6.1 Examples using MPI_SCATTER, MPI_SCATTERV

=== 6.7 Gather-to-all

==== 6.7.1 Example using MPI_ALLGATHER

=== 6.8 All-to-All Scatter/Gather

=== 6.9 Global Reduction Operations

==== 6.9.1 Reduce

==== 6.9.2 Predefined Reduction Operations

==== 6.9.3 Signed Characters and Reductions

==== 6.9.4 MINLOC and MAXLOC

==== 6.9.5 User-Defined Reduction Operations

===== 6.9.5.1 Example of User-Defined Reduce

==== 6.9.6 All-Reduce

==== 6.9.7 Process-Local Reduction

=== 6.10 Reduce-Scatter

==== 6.10.1 MPI_REDUCE_SCATTER_BLOCK

==== 6.10.2 MPI_REDUCE_SCATTER

=== 6.11 Scan

==== 6.11.1 Inclusive Scan

==== 6.11.2 Exclusive Scan

==== 6.11.3 Example using MPI_SCAN

=== 6.12 Nonblocking Collective Operations

==== 6.12.1 Nonblocking Barrier Synchronization

==== 6.12.2 Nonblocking Broadcast

===== 6.12.2.1 Example using MPI_IBCAST

==== 6.12.3 Nonblocking Gather

==== 6.12.4 Nonblocking Scatter

==== 6.12.5 Nonblocking Gather-to-all

==== 6.12.6 Nonblocking All-to-All Scatter/Gather

==== 6.12.7 Nonblocking Reduce

==== 6.12.8 Nonblocking All-Reduce

==== 6.12.9 Nonblocking Reduce-Scatter with Equal Blocks

==== 6.12.10 Nonblocking Reduce-Scatter

==== 6.12.11 Nonblocking Inclusive Scan

==== 6.12.12 Nonblocking Exclusive Scan

=== 6.13 Persistent Collective Operations

==== 6.13.1 Persistent Barrier Synchronization

==== 6.13.2 Persistent Broadcast

==== 6.13.3 Persistent Gather

==== 6.13.4 Persistent Scatter

==== 6.13.5 Persistent Gather-to-all

==== 6.13.6 Persistent All-to-All Scatter/Gather

==== 6.13.7 Persistent Reduce

==== 6.13.8 Persistent All-Reduce

==== 6.13.9 Persistent Reduce-Scatter with Equal Blocks

==== 6.13.10 Persistent Reduce-Scatter

==== 6.13.11 Persistent Inclusive Scan

==== 6.13.12 Persistent Exclusive Scan

=== 6.14 Correctness

== Chapter 7 Groups, Contexts, Communicators, and Caching

=== 7.1 Introduction

==== 7.1.1 Features Needed to Support Libraries

==== 7.1.2 MPI’s Support for Libraries

=== 7.2 Basic Concepts

==== 7.2.1 Groups

==== 7.2.2 Contexts

==== 7.2.3 Intra-Communicators

==== 7.2.4 Predefined Intra-Communicators

=== 7.3 Group Management

==== 7.3.1 Group Accessors

==== 7.3.2 Group Constructors

==== 7.3.3 Group Destructors

=== 7.4 Communicator Management

==== 7.4.1 Communicator Accessors

==== 7.4.2 Communicator Constructors

==== 7.4.3 Communicator Destructors

==== 7.4.4 Communicator Info

=== 7.5 Motivating Examples

==== 7.5.1 Current Practice #1

==== 7.5.2 Current Practice #2

==== 7.5.3 (Approximate) Current Practice #3

==== 7.5.4 Communication Safety Example

==== 7.5.5 Library Example #1

==== 7.5.6 Library Example #2

=== 7.6 Inter-Communication

==== 7.6.1 Inter-Communicator Accessors

==== 7.6.2 Inter-Communicator Operations

==== 7.6.3 Inter-Communication Examples

===== 7.6.3.1 Example 1: Three-Group "Pipeline"

===== 7.6.3.2 Example 2: Three-Group "Ring"

=== 7.7 Caching

==== 7.7.1 Functionality

==== 7.7.2 Communicators

==== 7.7.3 Windows

==== 7.7.4 Datatypes

==== 7.7.5 Error Class for Invalid Keyval

==== 7.7.6 Attributes Example

=== 7.8 Naming Objects

=== 7.9 Formalizing the Loosely Synchronous Model

==== 7.9.1 Basic Statements

==== 7.9.2 Models of Execution

===== 7.9.2.1 Static Communicator Allocation

===== 7.9.2.2 Dynamic Communicator Allocation

===== 7.9.2.3 The General Case

== Chapter 8 Process Topologies

=== 8.1 Introduction

=== 8.2 Virtual Topologies

=== 8.3 Embedding in MPI

=== 8.4 Overview of the Functions

=== 8.5 Topology Constructors

==== 8.5.1 Cartesian Constructor

==== 8.5.2 Cartesian Convenience Function: MPI_DIMS_CREATE

==== 8.5.3 Graph Constructor

==== 8.5.4 Distributed Graph Constructor

==== 8.5.5 Topology Inquiry Functions

==== 8.5.6 Cartesian Shift Coordinates

==== 8.5.7 Partitioning of Cartesian Structures

==== 8.5.8 Low-Level Topology Functions

=== 8.6 Neighborhood Collective Communication

==== 8.6.1 Neighborhood Gather

==== 8.6.2 Neighbor Alltoall

=== 8.7 Nonblocking Neighborhood Communication

==== 8.7.1 Nonblocking Neighborhood Gather

==== 8.7.2 Nonblocking Neighborhood Alltoall

=== 8.8 Persistent Neighborhood Communication

==== 8.8.1 Persistent Neighborhood Gather

==== 8.8.2 Persistent Neighborhood Alltoall

=== 8.9 An Application Example

== Chapter 9 MPI Environmental Management

=== 9.1 Implementation Information

==== 9.1.1 Version Inquiries

==== 9.1.2 Environmental Inquiries

===== 9.1.2.1 Tag Values

===== 9.1.2.2 Host Rank

===== 9.1.2.3 IO Rank

===== 9.1.2.4 Clock Synchronization

===== 9.1.2.5 Inquire Processor Name

=== 9.2 Memory Allocation

=== 9.3 Error Handling

==== 9.3.1 Error Handlers for Communicators

==== 9.3.2 Error Handlers for Windows

==== 9.3.3 Error Handlers for Files

==== 9.3.4 Error Handlers for Sessions

==== 9.3.5 Freeing Errorhandlers and Retrieving Error Strings

=== 9.4 Error Codes and Classes

=== 9.5 Error Classes, Error Codes, and Error Handlers

=== 9.6 Timers and Synchronization

== Chapter 10 The Info Object

== Chapter 11 Process Initialization, Creation, and Management

=== 11.1 Introduction

=== 11.2 The World Model

==== 11.2.1 Starting MPI Processes

==== 11.2.2 Finalizing MPI

==== 11.2.3 Determining Whether MPI Has Been Initialized When Using the World Model

==== 11.2.4 Allowing User Functions at MPI Finalization

=== 11.3 The Sessions Model

==== 11.3.1 Session Creation and Destruction Methods

==== 11.3.2 Processes Sets

==== 11.3.3 Runtime Query Functions

==== 11.3.4 Sessions Model Examples

=== 11.4 Common Elements of Both Process Models

==== 11.4.1 MPI Functionality that is Always Available

==== 11.4.2 Aborting MPI Processes

=== 11.5 Portable MPI Process Startup

=== 11.6 MPI and Threads

==== 11.6.1 General

==== 11.6.2 Clarifications

=== 11.7 The Dynamic Process Model

==== 11.7.1 Starting Processes

==== 11.7.2 The Runtime Environment

=== 11.8 Process Manager Interface

==== 11.8.1 Processes in MPI

==== 11.8.2 Starting Processes and Establishing Communication

==== 11.8.3 Starting Multiple Executables and Establishing Communication .

==== 11.8.4 Reserved Keys

==== 11.8.5 Spawn Example

=== 11.9 Establishing Communication

==== 11.9.1 Names, Addresses, Ports, and All That

==== 11.9.2 Server Routines

==== 11.9.3 Client Routines

==== 11.9.4 Name Publishing

==== 11.9.5 Reserved Key Values

==== 11.9.6 Client/Server Examples

=== 11.10 Other Functionality

==== 11.10.1 Universe Size

==== 11.10.2 Singleton MPI Initialization

==== 11.10.3 MPI_APPNUM

==== 11.10.4 Releasing Connections

==== 11.10.5 Another Way to Establish MPI Communication

== Chapter 12 One-Sided Communications

=== 12.1 Introduction

=== 12.2 Initialization

==== 12.2.1 Window Creation

==== 12.2.2 Window That Allocates Memory

==== 12.2.3 Window That Allocates Shared Memory

==== 12.2.4 Window of Dynamically Attached Memory

==== 12.2.5 Window Destruction

==== 12.2.6 Window Attributes

==== 12.2.7 Window Info

=== 12.3 Communication Calls

==== 12.3.1 Put

==== 12.3.2 Get

==== 12.3.3 Examples for Communication Calls

==== 12.3.4 Accumulate Functions

===== 12.3.4.1 Accumulate Function

===== 12.3.4.2 Get Accumulate Function

===== 12.3.4.3 Fetch and Op Function

===== 12.3.4.4 Compare and Swap Function

==== 12.3.5 Request-based RMA Communication Operations

=== 12.4 Memory Model

=== 12.5 Synchronization Calls

==== 12.5.1 Fence

==== 12.5.2 General Active Target Synchronization

==== 12.5.3 Lock

==== 12.5.4 Flush and Sync

==== 12.5.5 Assertions

==== 12.5.6 Miscellaneous Clarifications

=== 12.6 Error Handling

==== 12.6.1 Error Handlers

==== 12.6.2 Error Classes

=== 12.7 Semantics and Correctness

==== 12.7.1 Atomicity

==== 12.7.2 Ordering

==== 12.7.3 Progress

==== 12.7.4 Registers and Compiler Optimizations

=== 12.8 Examples

== Chapter 13 External Interfaces

=== 13.1 Introduction

=== 13.2 Generalized Requests

==== 13.2.1 Examples

=== 13.3 Associating Information with Status

== Chapter 14 I/O

=== 14.1 Introduction

==== 14.1.1 Definitions

=== 14.2 File Manipulation

==== 14.2.1 Opening a File

==== 14.2.2 Closing a File

==== 14.2.3 Deleting a File

==== 14.2.4 Resizing a File

==== 14.2.5 Preallocating Space for a File

==== 14.2.6 Querying the Size of a File

==== 14.2.7 Querying File Parameters

==== 14.2.8 File Info

===== 14.2.8.1 Reserved File Hints

=== 14.3 File Views

=== 14.4 Data Access

==== 14.4.1 Data Access Routines

===== 14.4.1.1 Positioning

===== 14.4.1.2 Synchronism

===== 14.4.1.3 Coordination

===== 14.4.1.4 Data Access Conventions

==== 14.4.2 Data Access with Explicit Offsets

==== 14.4.3 Data Access with Individual File Pointers

==== 14.4.4 Data Access with Shared File Pointers

===== 14.4.4.1 Noncollective Operations

===== 14.4.4.2 Collective Operations

===== 14.4.4.3 Seek

==== 14.4.5 Split Collective Data Access Routines

=== 14.5 File Interoperability

==== 14.5.1 Datatypes for File Interoperability

==== 14.5.2 External Data Representation: "external32"

==== 14.5.3 User-Defined Data Representations

===== 14.5.3.1 Extent Callback

===== 14.5.3.2 Datarep Conversion Functions

==== 14.5.4 Matching Data Representations

=== 14.6 Consistency and Semantics

==== 14.6.1 File Consistency

==== 14.6.2 Random Access vs. Sequential Files

==== 14.6.3 Progress

==== 14.6.4 Collective File Operations

==== 14.6.5 Nonblocking Collective File Operations

==== 14.6.6 Type Matching

==== 14.6.7 Miscellaneous Clarifications

==== 14.6.8 MPI_Offset Type

==== 14.6.9 Logical vs. Physical File Layout

==== 14.6.10 File Size

==== 14.6.11 Examples

===== 14.6.11.1 Asynchronous I/O

=== 14.7 I/O Error Handling

=== 14.8 I/O Error Classes

=== 14.9 Examples

==== 14.9.1 Double Buffering with Split Collective I/O

==== 14.9.2 Subarray Filetype Constructor

== Chapter 15 Tool Support

=== 15.1 Introduction

=== 15.2 Profiling Interface

==== 15.2.1 Requirements

==== 15.2.2 Discussion

==== 15.2.3 Logic of the Design

==== 15.2.4 Miscellaneous Control of Profiling

==== 15.2.5 MPI Library Implementation

==== 15.2.6 Complications

==== 15.2.7 Multiple Levels of Interception

=== 15.3 The MPI Tool Information Interface

==== 15.3.1 Verbosity Levels

==== 15.3.2 Binding MPI Tool Information Interface Variables to MPI Objects

==== 15.3.3 Convention for Returning Strings

==== 15.3.4 Initialization and Finalization

==== 15.3.5 Datatype System

==== 15.3.6 Control Variables

==== 15.3.7 Performance Variables

===== 15.3.7.1 Performance Variable Classes

===== 15.3.7.2 Performance Variable Query Functions

===== 15.3.7.3 Performance Experiment Sessions

===== 15.3.7.4 Handle Allocation and Deallocation

===== 15.3.7.5 Starting and Stopping of Performance Variables

===== 15.3.7.6 Performance Variable Access Functions

==== 15.3.8 Events

===== 15.3.8.1 Event Sources

===== 15.3.8.2 Callback Safety Requirements

===== 15.3.8.3 Event Type Query Functions

===== 15.3.8.4 Handle Allocation and Deallocation

===== 15.3.8.5 Handling Dropped Events

===== 15.3.8.6 Reading Event Data

===== 15.3.8.7 Reading Event Meta Data

==== 15.3.9 Variable Categorization

===== 13.3.9.1 Category Query Functions

===== 13.3.9.2 Category Member Query Functions

==== 15.3.10 Return Codes for the MPI Tool Information Interface

==== 15.3.11 Profiling Interface

== Chapter 16 Deprecated Interfaces

=== 16.1 Deprecated since MPI-2.0

=== 16.2 Deprecated since MPI-2.2
=== 16.3 Deprecated since MPI-4.0

== Chapter 17 Removed Interfaces

=== 17.1 Removed MPI-1 Bindings

==== 17.1.1 Overview

==== 17.1.2 Removed MPI-1 Functions

==== 17.1.3 Removed MPI-1 Datatypes

==== 17.1.4 Removed MPI-1 Constants

==== 17.1.5 Removed MPI-1 Callback Prototypes

=== 17.2 C++ Bindings

== Chapter 18 Semantic Changes and Warnings
=== 18.1 Semantic Changes

==== 18.1.1 Semantic Changes Starting in MPI-4.0

=== 18.2 Additional Warnings

==== 18.2.1 Warnings Starting in MPI-4.0

== Chapter 19 Language Bindings

=== 19.1 Support for Fortran

==== 19.1.1 Overview

==== 19.1.2 Fortran Support Through the mpi_f08 Module

==== 19.1.3 Fortran Support Through the mpi Module

==== 19.1.4 Fortran Support Through the mpif.h Include File

==== 19.1.5 Interface Specifications, Procedure Names, and the Profiling Interface798

==== 19.1.6 MPI for Different Fortran Standard Versions

==== 19.1.7 Requirements on Fortran Compilers

==== 19.1.8 Additional Support for Fortran Register-Memory-Synchronization 808

==== 19.1.9 Additional Support for Fortran Numeric Intrinsic Types

===== 19.1.9.1 Parameterized Datatypes with Specified Precision and Exponent

===== 19.1.9.2 Range

===== 19.1.9.3 Support for Size-specific MPI Datatypes

===== 19.1.9.4 Communication With Size-specific Types

==== 19.1.10 Problems With Fortran Bindings for MPI

==== 19.1.11 Problems Due to Strong Typing

==== 19.1.12 Problems Due to Data Copying and Sequence Association with Subscript Triplets

==== 19.1.13 Problems Due to Data Copying and Sequence Association with Vector Subscripts

==== 19.1.14 Special Constants

==== 19.1.15 Fortran Derived Types

==== 19.1.16 Optimization Problems, an Overview

==== 19.1.17 Problems with Code Movement and Register Optimization

===== 19.1.17.1 Nonblocking Operations

===== 19.1.17.2 Persistent Operations

===== 19.1.17.3 One-sided Communication

===== 19.1.17.4 MPI_BOTTOM and Combining Independent Variables in Datatypes 827

===== 19.1.17.5 Solutions

===== 19.1.17.6 The Fortran ASYNCHRONOUS Attribute

===== 19.1.17.7 Calling MPI_F_SYNC_REG

===== 19.1.17.8 A User Defined Routine Instead of MPI_F_SYNC_REG

===== 19.1.17.9 Module Variables and COMMON Blocks

===== 19.1.17.10 The (Poorly Performing) Fortran VOLATILE Attribute

===== 19.1.17.11 The Fortran TARGET Attribute

==== 19.1.18 Temporary Data Movement and Temporary Memory Modification 832

==== 19.1.19 Permanent Data Movement

==== 19.1.20 Comparison with C

=== 19.2 Support for Large Count and Large Byte Displacement

=== 19.3 Language Interoperability

==== 19.3.1 Introduction

==== 19.3.2 Assumptions

==== 19.3.3 Initialization

==== 19.3.4 Transfer of Handles

==== 19.3.5 Status

==== 19.3.6 MPI Opaque Objects

===== 19.3.6.1 Datatypes

===== 19.3.6.2 Callback Functions

===== 19.3.6.3 Error Handlers

===== 19.3.6.4 Reduce Operations

==== 19.3.7 Attributes

==== 19.3.8 Extra-State

==== 19.3.9 Constants

==== 19.3.10 Interlanguage Communication

== Chapter A Language Bindings Summary

=== A.1 Defined Values and Handles

==== A.1.1 Defined Constants

==== A.1.2 Types

==== A.1.3 Prototype Definitions

===== A.1.3.1 C Bindings

===== A.1.3.2 Fortran 2008 Bindings with the mpi_f08 Module

===== A.1.3.3 Fortran Bindings with mpif.h or the mpi Module

==== A.1.4 Deprecated Prototype Definitions

==== A.1.5 String Values

===== A.1.5.1 Default Communicator Names

===== A.1.5.2 Reserved Data Representations

===== A.1.5.3 Process Set Names

===== A.1.5.4 Info Keys

===== A.1.5.5 Info Values

=== A.2 Summary of the Semantics of all Op.-Related Routines

=== A.3 C Bindings

==== A.3.1 Point-to-Point Communication C Bindings

==== A.3.2 Partitioned Communication C Bindings

==== A.3.3 Datatypes C Bindings

==== A.3.4 Collective Communication C Bindings

==== A.3.5 Groups, Contexts, Communicators, and Caching C Bindings

==== A.3.6 Process Topologies C Bindings

==== A.3.7 MPI Environmental Management C Bindings

==== A.3.8 The Info Object C Bindings

==== A.3.9 Process Creation and Management C Bindings

==== A.3.10 One-Sided Communications C Bindings

==== A.3.11 External Interfaces C Bindings

==== A.3.12 I/O C Bindings

==== A.3.13 Language Bindings C Bindings

==== A.3.14 Tools / Profiling Interface C Bindings

==== A.3.15 Tools / MPI Tool Information Interface C Bindings

==== A.3.16 Deprecated C Bindings

=== A.4 Fortran 2008 Bindings with the mpi_f08 Module

==== A.4.1 Point-to-Point Communication Fortran 2008 Bindings

==== A.4.2 Partitioned Communication Fortran 2008 Bindings

==== A.4.3 Datatypes Fortran 2008 Bindings

==== A.4.4 Collective Communication Fortran 2008 Bindings

==== A.4.5 Groups, Contexts, Communicators, and Caching Fortran 2008 Bindings

==== A.4.6 Process Topologies Fortran 2008 Bindings

==== A.4.7 MPI Environmental Management Fortran 2008 Bindings

==== A.4.8 The Info Object Fortran 2008 Bindings

==== A.4.9 Process Creation and Management Fortran 2008 Bindings

==== A.4.10 One-Sided Communications Fortran 2008 Bindings

==== A.4.11 External Interfaces Fortran 2008 Bindings

==== A.4.12 I/O Fortran 2008 Bindings

==== A.4.13 Language Bindings Fortran 2008 Bindings

==== A.4.14 Tools / Profiling Interface Fortran 2008 Bindings

==== A.4.15 Deprecated Fortran 2008 Bindings

=== A.5 Fortran Bindings with mpif.h or the mpi Module

==== A.5.1 Point-to-Point Communication Fortran Bindings

==== A.5.2 Partitioned Communication Fortran Bindings

==== A.5.3 Datatypes Fortran Bindings

==== A.5.4 Collective Communication Fortran Bindings

==== A.5.5 Groups, Contexts, Communicators, and Caching Fortran Bindings 1020

==== A.5.6 Process Topologies Fortran Bindings

==== A.5.7 MPI Environmental Management Fortran Bindings

==== A.5.8 The Info Object Fortran Bindings

==== A.5.9 Process Creation and Management Fortran Bindings

==== A.5.10 One-Sided Communications Fortran Bindings

==== A.5.11 External Interfaces Fortran Bindings

==== A.5.12 I/O Fortran Bindings

==== A.5.13 Language Bindings Fortran Bindings

==== A.5.14 Tools / Profiling Interface Fortran Bindings

==== A.5.15 Deprecated Fortran Bindings

== Chapter B Change-Log

=== B.1 Changes from Version 3.1 to Version 4.0

==== B.1.1 Fixes to Errata in Previous Versions of MPI

==== B.1.2 Changes in MPI-4.0

=== B.2 Changes from Version 3.0 to Version 3.1

==== B.2.1 Fixes to Errata in Previous Versions of MPI

==== B.2.2 Changes in MPI-3.1

=== B.3 Changes from Version 2.2 to Version 3.0

==== B.3.1 Fixes to Errata in Previous Versions of MPI

==== B.3.2 Changes in MPI-3.0

=== B.4 Changes from Version 2.1 to Version 2.2

=== B.5 Changes from Version 2.0 to Version 2.1


== Chapter Bibliography

== Chapter General Index

== Chapter Examples Index

== Chapter MPI Constant and Predefined Handle Index

== Chapter MPI Declarations Index

== Chapter MPI Callback Function Prototype Index

== Chapter MPI Function Index

